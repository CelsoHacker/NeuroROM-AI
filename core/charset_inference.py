# -*- coding: utf-8 -*-
"""
================================================================================
CHARSET INFERENCE - Descoberta Autom√°tica de Tabelas de Caracteres
================================================================================
Infer√™ncia estat√≠stica de mapeamento byte ‚Üí caractere atrav√©s de:
- An√°lise de frequ√™ncia (compara√ß√£o com idiomas conhecidos)
- Detec√ß√£o de padr√µes (espa√ßos, terminadores, pontua√ß√£o)
- Machine learning de padr√µes lingu√≠sticos
- Refinamento iterativo

Gera tabelas candidatas sem conhecimento pr√©vio do jogo
================================================================================
"""

from typing import Dict, List, Tuple, Optional, Set
from collections import Counter, defaultdict
import json
from pathlib import Path


class CharsetCandidate:
    """Representa uma tabela de caracteres candidata."""

    def __init__(self, name: str):
        self.name = name
        self.byte_to_char: Dict[int, str] = {}
        self.char_to_byte: Dict[str, int] = {}
        self.confidence = 0.0
        self.evidence = []

    def add_mapping(self, byte: int, char: str, evidence: str):
        """Adiciona mapeamento byte ‚Üí char com evid√™ncia."""
        self.byte_to_char[byte] = char
        self.char_to_byte[char] = byte
        self.evidence.append({
            'byte': f'0x{byte:02X}',
            'char': char,
            'reason': evidence
        })

    def __repr__(self):
        return f"<CharsetCandidate '{self.name}' mappings={len(self.byte_to_char)} confidence={self.confidence:.2f}>"


class CharsetInferenceEngine:
    """
    Motor de infer√™ncia de tabelas de caracteres baseado em an√°lise estat√≠stica.
    """

    # Frequ√™ncias esperadas de letras em portugu√™s (%)
    PT_LETTER_FREQ = {
        'a': 14.63, 'e': 12.57, 'o': 10.73, 's': 7.81, 'r': 6.53,
        'i': 6.18, 'n': 5.05, 'd': 4.99, 'm': 4.74, 'u': 4.63,
        't': 4.34, 'c': 3.88, 'l': 2.78, 'p': 2.52, 'v': 1.67
    }

    # Frequ√™ncias em ingl√™s (para compara√ß√£o)
    EN_LETTER_FREQ = {
        'e': 12.70, 't': 9.06, 'a': 8.17, 'o': 7.51, 'i': 6.97,
        'n': 6.75, 's': 6.33, 'h': 6.09, 'r': 5.99, 'd': 4.25
    }

    # Bytes comuns para controle
    COMMON_CONTROL_BYTES = {
        0x00: '<NULL>',
        0x0A: '<LF>',
        0x0D: '<CR>',
        0xFF: '<END>',
        0xFE: '<WAIT>',
        0xFD: '<PAGE>'
    }

    def __init__(self, text_candidates: List):
        """
        Args:
            text_candidates: Lista de TextCandidate do TextScanner
        """
        self.text_candidates = text_candidates
        self.charset_candidates = []
        self.byte_frequency = Counter()
        self.byte_positions = defaultdict(list)  # byte ‚Üí [posi√ß√µes na string]

    def infer_charsets(self) -> List[CharsetCandidate]:
        """
        Executa processo completo de infer√™ncia de tabelas.

        Returns:
            Lista de tabelas candidatas ordenadas por confian√ßa
        """
        print(f"\nüî§ CHARSET INFERENCE - Automatic Character Table Discovery")
        print(f"{'='*70}")

        if not self.text_candidates:
            print("‚ùå No text candidates provided!")
            return []

        # Fase 1: An√°lise de frequ√™ncia
        print("[1/5] Analyzing byte frequencies...")
        self._analyze_byte_patterns()

        # Fase 2: Detecta bytes especiais (espa√ßo, terminadores)
        print("[2/5] Identifying control bytes...")
        space_byte, terminator_bytes = self._identify_special_bytes()

        # Fase 3: Infer√™ncia baseada em frequ√™ncia lingu√≠stica
        print("[3/5] Inferring character mappings...")
        charset_freq = self._infer_from_frequency(space_byte)

        # Fase 4: Infer√™ncia baseada em posi√ß√£o (primeira letra = mai√∫scula)
        print("[4/5] Refining with positional analysis...")
        charset_pos = self._infer_from_position(space_byte)

        # Fase 5: Combina evid√™ncias e gera tabelas finais
        print("[5/5] Generating candidate tables...")
        self.charset_candidates = self._generate_candidates(
            charset_freq, charset_pos, space_byte, terminator_bytes
        )

        # Ordena por confian√ßa
        self.charset_candidates.sort(key=lambda c: c.confidence, reverse=True)

        print(f"\n‚úÖ Generated {len(self.charset_candidates)} charset candidates")
        print(f"{'='*70}\n")

        return self.charset_candidates

    def _analyze_byte_patterns(self):
        """Analisa padr√µes de frequ√™ncia e posi√ß√£o de bytes."""
        for candidate in self.text_candidates:
            for i, byte in enumerate(candidate.data):
                self.byte_frequency[byte] += 1
                self.byte_positions[byte].append(i)

    def _identify_special_bytes(self) -> Tuple[Optional[int], Set[int]]:
        """
        Identifica bytes especiais (espa√ßo e terminadores) por heur√≠stica.

        Returns:
            (space_byte, terminator_bytes_set)
        """
        # Espa√ßo: geralmente o byte mais frequente ou segundo mais frequente
        most_common = self.byte_frequency.most_common(10)

        space_candidates = []
        for byte, count in most_common:
            # Espa√ßo tende a aparecer entre 5-20% do total
            frequency_ratio = count / sum(self.byte_frequency.values())
            if 0.05 <= frequency_ratio <= 0.20:
                space_candidates.append((byte, frequency_ratio))

        space_byte = space_candidates[0][0] if space_candidates else 0x20  # Default ASCII space

        # Terminadores: bytes que aparecem no final das strings
        terminator_candidates = Counter()
        for candidate in self.text_candidates:
            if candidate.data:
                last_byte = candidate.data[-1]
                terminator_candidates[last_byte] += 1

        # Terminadores mais comuns
        terminators = set()
        for byte, count in terminator_candidates.most_common(3):
            if count >= len(self.text_candidates) * 0.1:  # Pelo menos 10% das strings
                terminators.add(byte)

        print(f"   Identified space byte: 0x{space_byte:02X}")
        print(f"   Identified terminators: {[f'0x{b:02X}' for b in terminators]}")

        return space_byte, terminators

    def _infer_from_frequency(self, space_byte: int) -> CharsetCandidate:
        """
        Infer√™ncia baseada em frequ√™ncia de letras.

        Compara frequ√™ncia de bytes com frequ√™ncia de letras em portugu√™s/ingl√™s.
        """
        charset = CharsetCandidate("frequency_based")

        # Remove bytes de controle e espa√ßo
        filtered_freq = {
            byte: count for byte, count in self.byte_frequency.items()
            if byte not in self.COMMON_CONTROL_BYTES and byte != space_byte
        }

        # Ordena bytes por frequ√™ncia (mais comum primeiro)
        sorted_bytes = sorted(filtered_freq.items(), key=lambda x: x[1], reverse=True)

        # Ordena letras portuguesas por frequ√™ncia
        sorted_letters = sorted(self.PT_LETTER_FREQ.items(), key=lambda x: x[1], reverse=True)

        # Mapeia os N bytes mais comuns para as N letras mais comuns
        n = min(len(sorted_bytes), len(sorted_letters))

        for i in range(n):
            byte, byte_count = sorted_bytes[i]
            letter, letter_freq = sorted_letters[i]

            # Adiciona mapeamento
            charset.add_mapping(
                byte, letter,
                f"Frequency match: {byte_count} occurrences ‚âà {letter_freq:.2f}%"
            )

        # Adiciona espa√ßo
        charset.add_mapping(space_byte, ' ', "Identified as space byte")

        # Calcula confian√ßa baseado na correla√ß√£o de frequ√™ncias
        charset.confidence = self._calculate_frequency_correlation(sorted_bytes[:n], sorted_letters[:n])

        return charset

    def _calculate_frequency_correlation(self, bytes_freq: List[Tuple], letters_freq: List[Tuple]) -> float:
        """
        Calcula correla√ß√£o entre distribui√ß√µes de bytes e letras.

        Usa coeficiente de correla√ß√£o de Pearson simplificado.
        """
        if not bytes_freq or not letters_freq:
            return 0.0

        # Normaliza frequ√™ncias
        total_bytes = sum(count for _, count in bytes_freq)
        total_letters = sum(freq for _, freq in letters_freq)

        byte_probs = [count / total_bytes for _, count in bytes_freq]
        letter_probs = [freq / total_letters for _, freq in letters_freq]

        # Correla√ß√£o simples (quanto mais pr√≥ximas as distribui√ß√µes, melhor)
        n = min(len(byte_probs), len(letter_probs))
        correlation = sum(abs(byte_probs[i] - letter_probs[i]) for i in range(n)) / n

        # Inverte: menor diferen√ßa = maior confian√ßa
        return max(0.0, 1.0 - correlation * 2)

    def _infer_from_position(self, space_byte: int) -> CharsetCandidate:
        """
        Infer√™ncia baseada em posi√ß√£o dos bytes.

        Heur√≠stica: Bytes ap√≥s espa√ßos tendem a ser letras min√∫sculas.
                    Bytes no in√≠cio de strings tendem a ser mai√∫sculas.
        """
        charset = CharsetCandidate("position_based")

        # Analisa bytes que aparecem ap√≥s espa√ßos
        bytes_after_space = Counter()
        bytes_at_start = Counter()

        for candidate in self.text_candidates:
            data = candidate.data

            # Primeiro byte (pode ser mai√∫scula)
            if data:
                bytes_at_start[data[0]] += 1

            # Bytes ap√≥s espa√ßo (podem ser min√∫sculas)
            for i in range(len(data) - 1):
                if data[i] == space_byte:
                    bytes_after_space[data[i + 1]] += 1

        # Bytes mais comuns ap√≥s espa√ßo ‚Üí min√∫sculas
        common_after_space = bytes_after_space.most_common(15)
        lowercase_letters = list('aeiousrndmtclpv')  # Mais comuns em portugu√™s

        for i, (byte, count) in enumerate(common_after_space):
            if i < len(lowercase_letters):
                charset.add_mapping(
                    byte, lowercase_letters[i],
                    f"Common after space ({count} times)"
                )

        # Bytes comuns no in√≠cio ‚Üí mai√∫sculas
        common_at_start = bytes_at_start.most_common(15)
        uppercase_letters = list('AEIOUSRNDMTCLPV')

        for i, (byte, count) in enumerate(common_at_start):
            if i < len(uppercase_letters) and byte not in charset.byte_to_char:
                charset.add_mapping(
                    byte, uppercase_letters[i],
                    f"Common at string start ({count} times)"
                )

        charset.confidence = min(len(charset.byte_to_char) / 50.0, 1.0)  # Max 50 chars

        return charset

    def _generate_candidates(self, charset_freq: CharsetCandidate,
                            charset_pos: CharsetCandidate,
                            space_byte: int,
                            terminators: Set[int]) -> List[CharsetCandidate]:
        """
        Combina evid√™ncias de diferentes m√©todos para gerar tabelas finais.
        """
        candidates = []

        # Candidato 1: Baseado em frequ√™ncia pura
        candidates.append(charset_freq)

        # Candidato 2: Baseado em posi√ß√£o
        candidates.append(charset_pos)

        # Candidato 3: H√≠brido (combina os dois)
        hybrid = CharsetCandidate("hybrid")

        # Prioriza mapeamentos que aparecem em ambos
        for byte in set(charset_freq.byte_to_char.keys()) | set(charset_pos.byte_to_char.keys()):
            char_freq = charset_freq.byte_to_char.get(byte)
            char_pos = charset_pos.byte_to_char.get(byte)

            if char_freq == char_pos and char_freq:
                # Concord√¢ncia total ‚Üí alta confian√ßa
                hybrid.add_mapping(byte, char_freq, "Agreement between methods")
            elif char_freq:
                # Apenas em frequ√™ncia
                hybrid.add_mapping(byte, char_freq, "Frequency-based mapping")

        # Adiciona c√≥digos de controle conhecidos
        for byte, symbol in self.COMMON_CONTROL_BYTES.items():
            if byte in terminators:
                hybrid.add_mapping(byte, symbol, "Identified terminator")

        # Adiciona espa√ßo
        hybrid.add_mapping(space_byte, ' ', "Identified space")

        # Confian√ßa do h√≠brido = m√©dia dos componentes
        hybrid.confidence = (charset_freq.confidence + charset_pos.confidence) / 2

        candidates.append(hybrid)

        return candidates

    def export_tables(self, output_dir: str):
        """Exporta tabelas candidatas como JSON."""
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)

        for i, charset in enumerate(self.charset_candidates, 1):
            filename = output_path / f"charset_candidate_{i}_{charset.name}.json"

            # Converte para formato export√°vel
            table_data = {
                'name': charset.name,
                'confidence': round(charset.confidence, 3),
                'total_mappings': len(charset.byte_to_char),
                'byte_to_char': {
                    f'0x{byte:02X}': char
                    for byte, char in sorted(charset.byte_to_char.items())
                },
                'char_to_byte': {
                    char: f'0x{byte:02X}'
                    for char, byte in sorted(charset.char_to_byte.items())
                },
                'evidence': charset.evidence
            }

            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(table_data, f, indent=2, ensure_ascii=False)

            print(f"‚úÖ Exported: {filename}")

    def test_charset(self, charset: CharsetCandidate, sample_data: bytes) -> str:
        """
        Testa uma tabela decodificando dados de amostra.

        Returns:
            String decodificada
        """
        result = []
        for byte in sample_data:
            char = charset.byte_to_char.get(byte, f'[{byte:02X}]')
            result.append(char)
        return ''.join(result)

    def print_candidates(self, n: int = 3):
        """Exibe resumo das N melhores tabelas."""
        print(f"\nüìã TOP {n} CHARSET CANDIDATES")
        print(f"{'='*70}")

        for i, charset in enumerate(self.charset_candidates[:n], 1):
            print(f"\n{i}. {charset.name.upper()}")
            print(f"   Confidence: {charset.confidence:.3f}")
            print(f"   Mappings:   {len(charset.byte_to_char)}")
            print(f"   Sample mappings:")

            # Exibe 10 primeiros mapeamentos
            for byte, char in list(charset.byte_to_char.items())[:10]:
                print(f"      0x{byte:02X} ‚Üí '{char}'")

            # Testa com primeiro candidato de texto
            if self.text_candidates:
                sample = self.text_candidates[0].data[:50]
                decoded = self.test_charset(charset, sample)
                print(f"   Test decode: {decoded}")

        print(f"\n{'='*70}\n")


def infer_charset_from_rom(rom_path: str, text_candidates=None) -> CharsetInferenceEngine:
    """
    Fun√ß√£o de conveni√™ncia para infer√™ncia completa.

    Args:
        rom_path: Caminho da ROM
        text_candidates: Candidatos do TextScanner (ou None para scan autom√°tico)

    Returns:
        CharsetInferenceEngine com tabelas geradas
    """
    if text_candidates is None:
        # Importa e executa TextScanner
        from .text_scanner import scan_text_in_rom
        scanner = scan_text_in_rom(rom_path)
        text_candidates = scanner.candidates

    engine = CharsetInferenceEngine(text_candidates)
    engine.infer_charsets()
    engine.print_candidates(3)

    # Exporta tabelas
    output_dir = Path(rom_path).parent / 'inferred_charsets'
    engine.export_tables(str(output_dir))

    return engine


if __name__ == "__main__":
    import sys

    if len(sys.argv) < 2:
        print("Usage: python charset_inference.py <rom_file>")
        sys.exit(1)

    rom_file = sys.argv[1]
    infer_charset_from_rom(rom_file)
