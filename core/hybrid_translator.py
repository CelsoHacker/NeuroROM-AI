"""
Sistema H√≠brido de Tradu√ß√£o com Fallback Autom√°tico
====================================================

Usa Gemini (r√°pido) quando quota dispon√≠vel,
automaticamente muda para Ollama (lento mas ilimitado) quando quota esgotar.

Autor: ROM Translation Framework v5.3
"""

import logging
from typing import List, Tuple, Optional, Dict, Any
from enum import Enum

logger = logging.getLogger(__name__)


class TranslationMode(Enum):
    """Modos de tradu√ß√£o dispon√≠veis"""
    GEMINI = "gemini"
    OLLAMA = "ollama"
    AUTO = "auto"  # Fallback autom√°tico
    SMART = "smart"  # NOVO: Gemini para longos, Ollama para curtos (economiza quota)


class HybridTranslator:
    """
    Tradutor h√≠brido que alterna automaticamente entre Gemini e Ollama

    Estrat√©gia:
    1. Tenta Gemini (r√°pido, 1-2s por batch)
    2. Se quota esgotada ‚Üí Ollama (lento, 30s por batch, mas ilimitado)
    3. Salva estat√≠sticas de uso
    """

    def __init__(self, api_key: str = None, prefer_gemini: bool = True):
        """
        Inicializa tradutor h√≠brido

        Args:
            api_key: Google Gemini API key (opcional)
            prefer_gemini: Se True, usa Gemini primeiro (mais r√°pido)
        """
        self.api_key = api_key
        self.prefer_gemini = prefer_gemini
        self.current_mode = TranslationMode.GEMINI if prefer_gemini else TranslationMode.OLLAMA

        # Estat√≠sticas
        self.stats = {
            'gemini_requests': 0,
            'ollama_requests': 0,
            'gemini_failures': 0,
            'ollama_failures': 0,
            'fallback_switches': 0,
            'total_texts_translated': 0,
            'gemini_quota_saved': 0  # Textos que usaram Ollama para economizar Gemini
        }

        # Configura√ß√£o do modo SMART (economiza quota)
        self.smart_config = {
            'short_text_threshold': 50,  # Textos <= 50 chars v√£o para Ollama
            'dialog_keywords': ['?', '!', '...', '"', "'"],  # Di√°logos v√£o para Gemini
            'menu_keywords': ['START', 'CONTINUE', 'OPTIONS', 'EXIT', 'SAVE', 'LOAD'],
        }

        # Importa m√≥dulos conforme dispon√≠vel
        self.gemini_available = False
        self.ollama_available = False

        self._check_availability()

    def _check_availability(self):
        """Verifica quais servi√ßos est√£o dispon√≠veis"""
        # Testa Gemini
        try:
            from interface import gemini_api
            if gemini_api.GENAI_AVAILABLE:
                self.gemini_available = True
                logger.info("‚úÖ Gemini API dispon√≠vel")
            else:
                logger.warning("‚ö†Ô∏è Biblioteca google-generativeai n√£o instalada")
        except ImportError:
            logger.warning("‚ö†Ô∏è M√≥dulo gemini_api n√£o encontrado")

        # Testa Ollama
        try:
            import requests
            response = requests.get('http://127.0.0.1:11434/api/tags', timeout=2)
            if response.status_code == 200:
                self.ollama_available = True
                models = response.json().get('models', [])
                logger.info(f"‚úÖ Ollama dispon√≠vel ({len(models)} modelos)")
            else:
                logger.warning("‚ö†Ô∏è Ollama respondeu mas com erro")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Ollama n√£o dispon√≠vel: {e}")

    def translate_batch(
        self,
        texts: List[str],
        target_language: str = "Portuguese (Brazil)",
        mode: TranslationMode = TranslationMode.AUTO
    ) -> Tuple[List[str], bool, Optional[str]]:
        """
        Traduz batch de textos com fallback autom√°tico

        Args:
            texts: Lista de textos para traduzir
            target_language: Idioma de destino
            mode: AUTO = fallback autom√°tico, GEMINI/OLLAMA = for√ßa um espec√≠fico

        Returns:
            (tradu√ß√µes, sucesso, erro)
        """
        if not texts:
            return [], True, None

        # Modo for√ßado
        if mode == TranslationMode.GEMINI:
            return self._translate_with_gemini(texts, target_language)
        elif mode == TranslationMode.OLLAMA:
            return self._translate_with_ollama(texts, target_language)
        elif mode == TranslationMode.SMART:
            return self._translate_smart(texts, target_language)

        # Modo AUTO: tenta Gemini primeiro se preferido
        if self.prefer_gemini and self.gemini_available:
            translations, success, error = self._translate_with_gemini(texts, target_language)

            # Se funcionou, retorna
            if success:
                return translations, success, error

            # Se falhou por quota esgotada, tenta Ollama
            if error and ("quota" in error.lower() or "429" in error):
                logger.warning(f"‚ö†Ô∏è Gemini quota esgotada - mudando para Ollama")
                self.stats['fallback_switches'] += 1
                self.current_mode = TranslationMode.OLLAMA

                if self.ollama_available:
                    return self._translate_with_ollama(texts, target_language)
                else:
                    return translations, False, "‚ùå Gemini sem quota e Ollama indispon√≠vel"

            # Outro erro, retorna erro
            return translations, success, error

        # Se n√£o preferir Gemini ou Gemini indispon√≠vel, usa Ollama
        if self.ollama_available:
            return self._translate_with_ollama(texts, target_language)

        # Nenhum dispon√≠vel
        return texts, False, "‚ùå Nenhum servi√ßo de tradu√ß√£o dispon√≠vel"

    def _classify_text(self, text: str) -> str:
        """
        Classifica texto para decidir qual tradutor usar.

        Returns:
            'gemini' = texto complexo (di√°logo longo, precisa qualidade)
            'ollama' = texto simples (menu curto, Ollama resolve)
        """
        text_upper = text.upper()
        text_len = len(text)

        # Regra 1: Textos muito curtos (menus) ‚Üí Ollama
        if text_len <= self.smart_config['short_text_threshold']:
            # Mas se tem pontua√ß√£o de di√°logo, pode ser importante
            has_dialog = any(kw in text for kw in self.smart_config['dialog_keywords'])
            if not has_dialog:
                return 'ollama'

        # Regra 2: Keywords de menu ‚Üí Ollama (s√£o simples)
        for kw in self.smart_config['menu_keywords']:
            if kw in text_upper:
                return 'ollama'

        # Regra 3: Texto longo com pontua√ß√£o de di√°logo ‚Üí Gemini (precisa qualidade)
        if text_len > 80:
            return 'gemini'

        # Regra 4: M√∫ltiplas frases ‚Üí Gemini
        if text.count('.') >= 2 or text.count('!') >= 2 or text.count('?') >= 1:
            return 'gemini'

        # Default: textos m√©dios v√£o para Ollama (economiza quota)
        return 'ollama'

    def _translate_smart(
        self,
        texts: List[str],
        target_language: str
    ) -> Tuple[List[str], bool, Optional[str]]:
        """
        Modo SMART: Economiza quota do Gemini.

        - Textos curtos/simples ‚Üí Ollama (gr√°tis, ilimitado)
        - Textos longos/di√°logos ‚Üí Gemini (qualidade, mas usa quota)
        """
        if not texts:
            return [], True, None

        # Classifica cada texto
        gemini_texts = []
        gemini_indices = []
        ollama_texts = []
        ollama_indices = []

        for i, text in enumerate(texts):
            classification = self._classify_text(text)
            if classification == 'gemini' and self.gemini_available and self.api_key:
                gemini_texts.append(text)
                gemini_indices.append(i)
            else:
                ollama_texts.append(text)
                ollama_indices.append(i)

        logger.info(f"üìä SMART: {len(gemini_texts)} para Gemini, {len(ollama_texts)} para Ollama")

        # Prepara resultado final
        translations = [''] * len(texts)
        all_success = True
        errors = []

        # Traduz com Ollama (textos simples)
        if ollama_texts and self.ollama_available:
            ollama_result, ollama_success, ollama_error = self._translate_with_ollama(
                ollama_texts, target_language
            )
            if ollama_success:
                for idx, trans in zip(ollama_indices, ollama_result):
                    translations[idx] = trans
                self.stats['gemini_quota_saved'] += len(ollama_texts)
            else:
                all_success = False
                if ollama_error:
                    errors.append(f"Ollama: {ollama_error}")
                # Fallback: mant√©m originais
                for idx, orig in zip(ollama_indices, ollama_texts):
                    translations[idx] = orig

        # Traduz com Gemini (textos complexos)
        if gemini_texts and self.gemini_available:
            gemini_result, gemini_success, gemini_error = self._translate_with_gemini(
                gemini_texts, target_language
            )
            if gemini_success:
                for idx, trans in zip(gemini_indices, gemini_result):
                    translations[idx] = trans
            else:
                # Gemini falhou - tenta Ollama como fallback
                if "quota" in str(gemini_error).lower() or "429" in str(gemini_error):
                    logger.warning("‚ö†Ô∏è Gemini quota esgotada - usando Ollama para textos complexos")
                    self.stats['fallback_switches'] += 1
                    if self.ollama_available:
                        fallback_result, fb_success, fb_error = self._translate_with_ollama(
                            gemini_texts, target_language
                        )
                        if fb_success:
                            for idx, trans in zip(gemini_indices, fallback_result):
                                translations[idx] = trans
                        else:
                            all_success = False
                            for idx, orig in zip(gemini_indices, gemini_texts):
                                translations[idx] = orig
                else:
                    all_success = False
                    if gemini_error:
                        errors.append(f"Gemini: {gemini_error}")
                    for idx, orig in zip(gemini_indices, gemini_texts):
                        translations[idx] = orig

        error_msg = "; ".join(errors) if errors else None
        return translations, all_success, error_msg

    def _translate_with_gemini(
        self,
        texts: List[str],
        target_language: str
    ) -> Tuple[List[str], bool, Optional[str]]:
        """Traduz usando Google Gemini"""
        if not self.gemini_available:
            return texts, False, "Gemini n√£o dispon√≠vel"

        if not self.api_key:
            return texts, False, "API Key do Gemini n√£o configurada"

        try:
            from interface import gemini_api

            logger.info(f"üöÄ Traduzindo {len(texts)} textos com Gemini...")

            translations, success, error = gemini_api.translate_batch(
                texts,
                self.api_key,
                target_language
            )

            if success:
                self.stats['gemini_requests'] += 1
                self.stats['total_texts_translated'] += len(texts)
                logger.info(f"‚úÖ Gemini: {len(texts)} textos traduzidos")
            else:
                self.stats['gemini_failures'] += 1
                logger.error(f"‚ùå Gemini falhou: {error}")

            return translations, success, error

        except Exception as e:
            self.stats['gemini_failures'] += 1
            error_msg = f"Erro ao usar Gemini: {str(e)}"
            logger.exception(error_msg)
            return texts, False, error_msg

    def _translate_with_ollama(
        self,
        texts: List[str],
        target_language: str,
        model: str = "llama3.2:3b"
    ) -> Tuple[List[str], bool, Optional[str]]:
        """Traduz usando Ollama local"""
        if not self.ollama_available:
            return texts, False, "Ollama n√£o dispon√≠vel"

        try:
            import requests
            from concurrent.futures import ThreadPoolExecutor, as_completed

            logger.info(f"‚ö° Traduzindo {len(texts)} textos com Ollama ({model}) - MODO PARALELO...")

            translations = [None] * len(texts)  # Pr√©-aloca lista

            def translate_single(index, text):
                """Traduz um √∫nico texto (fun√ß√£o interna para paralelismo)"""
                # Prompt em INGL√äS (Llama responde melhor) pedindo tradu√ß√£o para PT-BR
                # CR√çTICO: Instru√ß√£o direta e clara para for√ßar tradu√ß√£o completa
                prompt = f"""You are a professional game translator. Translate the following text to Brazilian Portuguese.

RULES:
- Translate EVERYTHING to Portuguese. Do NOT leave any English words.
- Keep the character's personality and tone (sarcastic, grumpy, angry, etc.)
- Preserve ONLY technical codes like {{VAR}}, [NAME], <0A>
- Output ONLY the translation. No explanations, no comments.

English: {text}
Portuguese:"""

                payload = {
                    "model": model,
                    "prompt": prompt,
                    "stream": False,
                    "options": {
                        "temperature": 0.3,  # Mais determin√≠stico
                        "num_predict": 256,  # AUMENTADO: permite respostas longas
                        "num_ctx": 1024,  # Contexto maior para textos longos
                        "top_p": 0.9,
                        "repeat_penalty": 1.1  # Evita repeti√ß√µes
                    }
                }

                try:
                    response = requests.post(
                        'http://127.0.0.1:11434/api/generate',
                        json=payload,
                        timeout=90
                    )

                    if response.status_code == 200:
                        result = response.json()
                        translation = result['response'].strip()
                        if '\n\n' in translation:
                            translation = translation.split('\n\n')[0]
                        return index, translation + '\n'
                    else:
                        return index, text + '\n'

                except Exception:
                    return index, text + '\n'

            # PROCESSAMENTO PARALELO com 8 workers
            with ThreadPoolExecutor(max_workers=8) as executor:
                futures = {executor.submit(translate_single, i, text): i for i, text in enumerate(texts)}

                for future in as_completed(futures):
                    index, translation = future.result()
                    translations[index] = translation

            # Verifica se traduziu tudo
            success = all(t is not None for t in translations)

            if success:
                self.stats['ollama_requests'] += 1
                self.stats['total_texts_translated'] += len(texts)
                logger.info(f"‚úÖ Ollama: {len(texts)} textos traduzidos")
            else:
                self.stats['ollama_failures'] += 1
                logger.error(f"‚ùå Ollama: tradu√ß√£o incompleta")

            return translations, success, None

        except Exception as e:
            self.stats['ollama_failures'] += 1
            error_msg = f"Erro ao usar Ollama: {str(e)}"
            logger.exception(error_msg)
            return [t + '\n' for t in texts], False, error_msg

    def get_stats(self) -> Dict[str, Any]:
        """Retorna estat√≠sticas de uso"""
        total_requests = self.stats['gemini_requests'] + self.stats['ollama_requests']

        gemini_percent = (self.stats['gemini_requests'] / total_requests * 100) if total_requests > 0 else 0
        ollama_percent = (self.stats['ollama_requests'] / total_requests * 100) if total_requests > 0 else 0

        return {
            **self.stats,
            'total_requests': total_requests,
            'gemini_percent': gemini_percent,
            'ollama_percent': ollama_percent,
            'current_mode': self.current_mode.value,
            'gemini_available': self.gemini_available,
            'ollama_available': self.ollama_available
        }

    def get_status_message(self) -> str:
        """Retorna mensagem de status formatada"""
        stats = self.get_stats()

        if stats['current_mode'] == 'gemini':
            mode_icon = "‚ö°"
            mode_text = "Gemini (R√°pido)"
        else:
            mode_icon = "üêå"
            mode_text = "Ollama (Lento)"

        return (
            f"{mode_icon} Modo: {mode_text} | "
            f"Textos traduzidos: {stats['total_texts_translated']} | "
            f"Gemini: {stats['gemini_requests']} | "
            f"Ollama: {stats['ollama_requests']} | "
            f"Fallbacks: {stats['fallback_switches']}"
        )

    def force_mode(self, mode: TranslationMode):
        """For√ßa um modo espec√≠fico"""
        self.current_mode = mode
        logger.info(f"üîß Modo for√ßado: {mode.value}")

    def reset_stats(self):
        """Reseta estat√≠sticas"""
        for key in self.stats:
            self.stats[key] = 0
        logger.info("üìä Estat√≠sticas resetadas")


# ============================================================================
# FUN√á√ïES HELPER
# ============================================================================

def translate_with_auto_fallback(
    texts: List[str],
    api_key: str,
    target_language: str = "Portuguese (Brazil)"
) -> Tuple[List[str], bool, Optional[str]]:
    """
    Traduz com fallback autom√°tico Gemini ‚Üí Ollama

    Args:
        texts: Textos para traduzir
        api_key: Gemini API key
        target_language: Idioma de destino

    Returns:
        (tradu√ß√µes, sucesso, erro)
    """
    translator = HybridTranslator(api_key=api_key, prefer_gemini=True)
    return translator.translate_batch(texts, target_language, TranslationMode.AUTO)


# ============================================================================
# TESTE
# ============================================================================

if __name__ == "__main__":
    import os

    logging.basicConfig(
        level=logging.INFO,
        format='[%(asctime)s] %(levelname)s - %(message)s',
        datefmt='%H:%M:%S'
    )

    print("\n" + "="*70)
    print("üß™ TESTE DO TRADUTOR H√çBRIDO")
    print("="*70 + "\n")

    # API Key (voc√™ pode passar como argumento)
    api_key = os.getenv('GEMINI_API_KEY', '')

    if not api_key:
        print("‚ö†Ô∏è GEMINI_API_KEY n√£o configurada - testando s√≥ Ollama")

    # Textos de teste
    test_texts = [
        "Welcome to the game!",
        "Press START to begin",
        "Game Over"
    ]

    # Cria tradutor
    translator = HybridTranslator(api_key=api_key, prefer_gemini=True)

    print(f"Disponibilidade:")
    print(f"  Gemini: {'‚úÖ' if translator.gemini_available else '‚ùå'}")
    print(f"  Ollama: {'‚úÖ' if translator.ollama_available else '‚ùå'}")
    print()

    # Traduz
    translations, success, error = translator.translate_batch(test_texts)

    print("\n" + "="*70)
    print("RESULTADOS:")
    print("="*70)

    for orig, trad in zip(test_texts, translations):
        print(f"  {orig:40} ‚Üí {trad.strip()}")

    print("\n" + "="*70)
    print("ESTAT√çSTICAS:")
    print("="*70)
    print(translator.get_status_message())
    print()

    stats = translator.get_stats()
    print(f"Requisi√ß√µes Gemini: {stats['gemini_requests']}")
    print(f"Requisi√ß√µes Ollama: {stats['ollama_requests']}")
    print(f"Fallbacks autom√°ticos: {stats['fallback_switches']}")
    print(f"Taxa Gemini: {stats['gemini_percent']:.1f}%")
    print(f"Taxa Ollama: {stats['ollama_percent']:.1f}%")
    print("="*70 + "\n")
